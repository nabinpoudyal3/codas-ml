{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "Unsupervised machine learning is a type of learning that occurs when we only have features $X = \\{X_{ji}\\}$ but not labels $y_j$. \n",
    "\n",
    "These algorithms typically revolve around clustering, i.e. assigning each row $X_{j \\cdot}$ to a cluster $C_k$ such that the rows that share a cluster are more *similar* than ones from different clusters.  There are several ways to do this, some which require ground truth labeling and some which do not.  \n",
    "\n",
    "Variations of unsupervised algorithms are often used for dimension reduction, example: random projections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics for Unsupervised Learning\n",
    "\n",
    "The simplest form is **Within Class Sum of Squares (WCSS)**, which is based on the 2nd moment of each cluster (mean centroids).  Let $\\mu_k$ be the mean centroids of all rows in cluster $k$, i.e. for all elements $X_{j \\cdot} \\in C_k$,\n",
    "$$\\mu_k = \\frac{1}{|C_k|} \\sum_{X_{j \\cdot} \\in C_k} X_{j \\cdot}.$$\n",
    "Then the WCSS is defined as the corresponding sum for each cluster:\n",
    "$$\\sum_k \\frac{1}{|C_k|} \\sum_{X_{j \\cdot} \\in C_k} \\| X_{j \\cdot} - \\mu_k\\|^2_2\\,.$$\n",
    "\n",
    "Another metric is called the **Silhouette Coefficient**.  If $a_j$ is the mean distance between a point $X_{j \\cdot}$ and the other points in the same cluster $C_k$ and $b_j$ is the mean is the distance between $X_{j \\cdot}$ and the other points in the next nearest cluster $C_k'$, then the coefficient is given by\n",
    "$$ \\frac{b-a}{\\max(a, b)}\\,. $$\n",
    "\n",
    "Assuming you have ground truth values $\\tilde C_k$ and predicted classification labels $C_k$, it is easy to calculate their **Mutual Information**.  Mutual information has deep ties to information theory [see the Wikipedia article](http://en.wikipedia.org/wiki/Mutual_Information).  If $N$ is the total number of samples (number of rows of $X$), we can define the probabilities\n",
    "$$ P_k = \\frac{|C_k|}{N} \\qquad \\tilde P_k = \\frac{|\\tilde C_k|}{N} \\qquad P_{k,l} = \\frac{|C_k \\cap \\tilde C_l|}{N} $$\n",
    "and then the mutual information is defined as\n",
    "$$ \\sum_{k, l} P_{k,l} \\log\\left(\\frac{P_{k,l}}{ P_k \\tilde P_l }\\right)\\,. $$\n",
    "\n",
    "For more information on evaluation metrics for clustering algorithms, checkout the [Scikit Learn page on Clustering Metrics](http://scikit-learn.org/stable/modules/clustering.html#clustering-evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score 0.351641137588\n",
      "Adjusted Mutual Information  0.560826030402\n",
      "Mutual Information  0.686961576597\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets\n",
    "\n",
    "# Fit KMeans onto the Iris dataset\n",
    "dataset = datasets.load_iris()\n",
    "X = dataset.data\n",
    "k_means = KMeans().fit(X)\n",
    "y = k_means.predict(X)\n",
    "\n",
    "# Compute Silhoutte score on clusters:\n",
    "print \"Silhouette Score\", metrics.silhouette_score(X, y, metric='euclidean')\n",
    "\n",
    "# Entropy\n",
    "labels_true = [1, 1, 0, 1, 2, 2, 1, 2, 0]\n",
    "labels_pred = [1, 0, 1, 1, 2, 1, 1, 2, 2]\n",
    "\n",
    "print \"Adjusted Mutual Information \", metrics.adjusted_mutual_info_score(labels_true, labels_pred)\n",
    "print \"Mutual Information \", metrics.mutual_info_score(labels_true, labels_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "## $K$-Means\n",
    "The specification of $K$-means is simple: assign a collection of clusters $C_k$ that minimize the distances from samples to cluster centroids:\n",
    "\n",
    "$$ \\mbox{argmin}_C \\sum_{k=1}^K \\sum_{X_j \\in C_k} \\| X_{j\\cdot} - \\mu_k \\|_2^2 $$\n",
    "\n",
    "where $\\mu_k$ is the center of the points in $C_k$.  The algorithm to implement this is simple:\n",
    "\n",
    "1. Initialize $\\mu_k$ (with possibly random values).  \n",
    "\n",
    "Then iterate between:\n",
    "   1. Assign $X_{j\\cdot}$ to the cluster $C_k$ that minimizes $\\|X_{j\\cdot} - \\mu_k\\|_2^2$.\n",
    "   1. Recompute $\\mu_k$ by averaging over all the points $X_{j\\cdot}$ in the cluster $C_k$.\n",
    "\n",
    "Notice that both iterative steps lower the objective (the algorithm is greedy) and there are only a finite number of possible partisions of the points $X_{j\\cdot}$ so the algorithm is gauranteed to converge.  The converged solution may not be globally optimal.\n",
    "\n",
    "## Gaussian Mixture Models\n",
    "We can generalize the notion of $K$-Means in two ways:\n",
    "1. Instead of requiring that each $X_{j\\cdot}$ strictly belong to a single $C_k$, we say it belongs to $C_k$ with a probability\n",
    "1. Insteading of just having $\\mu_k$ as a degree of freedom, we have the pair $(\\mu_k, \\Sigma_k)$\n",
    "\n",
    "Then each $X_{j\\cdot}$ is of type $k$ with probability proportional to\n",
    "$$ p_k \\exp \\left( \\frac{1}{2} (X_{j\\cdot} - \\mu_k) \\cdot \\Sigma_k^{-1}(X_{j\\cdot} - \\mu_k) \\right) $$\n",
    "where $p_k > 0$ is a proportionality constant.\n",
    "While this is not strictly a clustering algorithm, it can be turned into one but choosing the probability cluster with the highest probability.\n",
    "\n",
    "**Question**:\n",
    "1. For KMeans, what happens if the features have very different scales?\n",
    "1. Does this problem exist for Gaussian Mixture Models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x10e456510>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAFCCAYAAAAKd53gAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7pLUQP8fcUhCSce2SK4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10de25cd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from sklearn import cluster, mixture, datasets\n",
    "import pandas as pd\n",
    "\n",
    "# load Boston dataset\n",
    "diabetes = datasets.load_boston()\n",
    "\n",
    "columns = [\"CRIM\",\"ZN\",\"INDUS\",\"CHAS\",\"NOX\",\"RM\",\"AGE\",\"DIS\",\"RAD\",\"TAX\",\"PTRATIO\",\"B\",\"LSAT\"]\n",
    "X = pd.DataFrame(diabetes.data, columns=columns)\n",
    "y = pd.Series(diabetes.target)\n",
    "\n",
    "gmm_clf = mixture.GaussianMixture(n_components=3, random_state=42).fit(X)\n",
    "kmeans_clf = cluster.KMeans(n_clusters=3, random_state=42).fit(X)\n",
    "\n",
    "# Compute the Silhouette\n",
    "pd.DataFrame([\n",
    "    (\"Gaussian MM\",  metrics.silhouette_score(X, gmm_clf.predict(X), metric='euclidean')) ,\n",
    "    (\"KMeans\",  metrics.silhouette_score(X, kmeans_clf.labels_, metric='euclidean'))\n",
    "], columns=[\"Model\", \"Silhouette\"]).plot(x=\"Model\", y=\"Silhouette\", kind=\"bar\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**:\n",
    "1. Normalize KMeans to improve performance use the scale function\n",
    "1. Test the potential energy criterion from earlier and graph it in the above plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark ML\n",
    "\n",
    "Spark ML is a Spark's machine learning library based on Dataframes (it superceeds MLlib, which is based on RDDs, and now being deprecated). Some parts of Spark ML - for instance, linear algebra - still call to RDD based API, BLAS wrappers.\n",
    "\n",
    "Spark ML provides a standardized API for machine learning algorithms to make it easier to combine multiple algorithms into a single pipeline or workflow. Similarly to what you already know about `scikit-learn`.\n",
    "\n",
    "## Basic components of a Pipeline\n",
    "\n",
    "Basic components of the Pipeline (in the scikit-learn sense) are:\n",
    " 1. Transformer: an abstract class to apply a transformation to dataset/dataframes\n",
    "    1. `UnaryTransformer` abstract class: takes an input column, applies transformation, and output the result as a new column. The input and output columns can be arbitrary. However, in the Pipeline input column should be `features` output column should be named `prediction`.\n",
    "    1. `UnaryTransofmer` has a `transform()` method\n",
    "\n",
    " 1. Estimator: implements an algorithm which can be `fit` to a dataframe. For instance: a learning algorithm is an Estimator which is trained on a dataframe to produce a model.  \n",
    "    1. Has a `fit()` method\n",
    "\n",
    "1. Parameter: an API to pass parameters to Transformers and Estimators  \n",
    "\n",
    "In Spark ML, `KMeans` algorithm is implemented as an Estimator which generates a `KMeansModel` as the base model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.clustering import BisectingKMeans, KMeans, GaussianMixture\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, NGram, IDF, StopWordsRemover\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we are going to use for this exercise comes from **Kaggle**: https://www.kaggle.com/datasets/?sortBy=votes&group=featured\n",
    "\n",
    "It contains news articles on 2 topics, with associated label. We do not ahve to download it ourselves, as I have done it for you (along with some additional preprocessing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_df = pd.read_csv(\"data/Articles.csv\",sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CleanedArticle</th>\n",
       "      <th>NewsType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KARACHI: The Sindh government has decided to b...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HONG KONG: Asian markets started 2015 on an up...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HONG KONG: Hong Kong shares opened 0.66 percen...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HONG KONG: Asian markets tumbled Tuesday follo...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NEW YORK: US oil prices Monday slipped below $...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      CleanedArticle  NewsType\n",
       "0  KARACHI: The Sindh government has decided to b...  business\n",
       "1  HONG KONG: Asian markets started 2015 on an up...  business\n",
       "2  HONG KONG: Hong Kong shares opened 0.66 percen...  business\n",
       "3  HONG KONG: Asian markets tumbled Tuesday follo...  business\n",
       "4  NEW YORK: US oil prices Monday slipped below $...  business"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CleanedArticle</th>\n",
       "      <th>NewsType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2692</td>\n",
       "      <td>2692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2584</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>strong&gt;KARACHI: Pakistan stocks ended at a rec...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>1408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           CleanedArticle NewsType\n",
       "count                                                2692     2692\n",
       "unique                                               2584        2\n",
       "top     strong>KARACHI: Pakistan stocks ended at a rec...   sports\n",
       "freq                                                    5     1408"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = spark.read.options(header=True,inferSchema=True,delimiter=\"|\").csv(\"file:///Users/alexey/Desktop/codas-ml/data/Articles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CleanedArticle: string (nullable = true)\n",
      " |-- NewsType: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|      CleanedArticle|NewsType|\n",
      "+--------------------+--------+\n",
      "|KARACHI: The Sind...|business|\n",
      "|\"HONG KONG: Asian...|business|\n",
      "|HONG KONG: Hong K...|business|\n",
      "|\"HONG KONG: Asian...|business|\n",
      "|\"NEW YORK: US oil...|business|\n",
      "+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use a standard natural language processing workflow for feature extraction, followed by the k-means clustering into 3 clusters.\n",
    "\n",
    "\n",
    "### Step 1.1.  Features: Bag of words (and variants)\n",
    "\n",
    "Learning algorithms like vectors of numbers, not text.  The simplest way to turn a text into a vector of number is to treat the text as a \"bag of words.\"  That is you\n",
    "\n",
    "  - Split the text into words\n",
    "  - Count how many times each word (/each word in some fixed vocabulary) occurs\n",
    "  - _(Optionally)_ normalize the counts against some baseline\n",
    "  - _(Variant)_ Just do a binary \"yes / no\" for whether each word (/.. in some vocabulary) is contained in the material\n",
    "  \n",
    "The output is a very large, but usually sparse, vector: The number of coordinates is the number of words in our dictionary, and the $i$-th coordinate entry is the number of occurances of the $i$-th word.\n",
    "\n",
    "There's a reasonable implementation of this in the CountVectorizer class in `sklearn.feature_extraction.text`.  See http://scikit-learn.org/stable/modules/classes.html#text-feature-extraction-ref for more detail on the options.\n",
    "\n",
    "\n",
    "#### Variation: Feature hashing\n",
    "\n",
    "When doing \"bag of words\" type techniques on a *large* corpus and without an existing vocabulary, there is a simple trick that is often useful.  The issue (and solution) is as follows: \n",
    "\n",
    " - The output is a feature vector, so that whenever we encounter a word we must look up which coordinate slot it is in.  A naive way would be to keep a list of all the words encoutered so far, and look up each word when it is encountered.  Whenever we encounter a new word, we see if we've already seen it before and if not -- assign it a new number.  This requires storing all the words that we have seen in memory, cannot be done in parallel (because we'd have to share the hash-table of seen words), etc.\n",
    " - A **hash function** takes as input something complicated (like a string) and spits out a number, with the desired property being that different inputs *usually* produce different outputs.  (This is how hash tables are implemented, as the name suggests.)\n",
    " - So -- rather than exactly looking up the coordinate of a given word, we can just use its hash value (modulo a big size that we choose).  This is fast and parallelizes easily.  (There are some downsides: You cannot tell, after the fact, what word each of your feature actually corresponds to!)\n",
    " \n",
    "Scikit-learn includes `sklearn.feature_extraction.text.HashingVectorizer` to do this.  It behaves as almost a drop-in replacement for `CountVectorizer`.  It can be used with tf-idf by combining it with the `TfidfTransformer` (the `TfidfVectorizer` is the `CountVectorizer` together with the `TfidfTransformer`). For our application (where the training and test data is small), we may as well just use `TfidfVectorizer` -- but it is good to know that `HashingVectorizer` is there.\n",
    "\n",
    "\n",
    "### Step 1.2 Stop words\n",
    "It's common to want to __omit__ certain common words when doing these counts -- \"a\", \"an\", and \"the\" are common enough so that their counts do not tend to give us any hints as to the meaning of documents.  Such words that we want to omit are called __stop words__ (they don't stop anything, though).\n",
    "\n",
    "NLTK contains a standard list of such stop words for English in `nltk.corpus.stopwords.words('english')`.  In our application, we'd also want to include \"apple\" -- it is certainly not going to help us distinguish our two meanings!\n",
    "\n",
    "### Step 1.3 n-grams\n",
    "\n",
    "Instead of looking at just single words, it is also useful to look at **n-grams**: These are n-word long sequences of words (i.e., each of \"farmer's market\", \"market share\", and \"farm share\" is a 2-gram).\n",
    "\n",
    "The exact same sort of counting techniques apply.  The `CountVectorizer` function has built in support for this, too:\n",
    "\n",
    "If you pass it the `ngram_range=(m, M)` then it will count $n$-grams with  $m \\leq n \\leq M$.\n",
    "\n",
    "### Step 1.4 TF-IDF: term frequency–inverse document frequency\n",
    "\n",
    "With single word vocabularies, we can probably do an okay job of coming up with a reasonable (if short) list of words that distinguish between the two documents.  With n-grams, even for $n=2$, it is better to let a computer help us.  \n",
    "\n",
    "Just using frequencies, as above, is clearly not great.  Both apples the fruit and Apple the company are enjoyed around the world (one of the 2-grams that came up above!).  We would like to find words that are common in one document, not not common in all of them.  This is the goal of the __td-idf weighting__.  A precise definition is:\n",
    "\n",
    "  1. If $d$ denotes a document and $t$ denotes a term, then the _raw term frequency_ $\\mathrm{tf}^{raw}(t,d)$ is\n",
    "  $$ \\mathrm{tf}^{raw}(t,d) = \\text{the number of times the term $t$ occurs in the document $d$} $$\n",
    "  The vector of all term frequencies can optionally be _normalized_ either by dividing by the maximum of ny single word's occurance count ($L^1$) or by the Euclidean length of the vector of word occurance counts ($L^2$).  Scikit-learn by defaults does this second one:\n",
    "  $$ \\mathrm{tf}(t,d) = \\mathrm{tf}^{L^2}(t,d) = \\frac{\\mathrm{tf}^{raw}(t,d)}{\\sqrt{\\sum_t \\mathrm{tf}^{raw}(t,d)^2}} $$\n",
    "  2. If $$ D = \\left\\{ d : d \\in D \\right\\} $$ is the set of possible documents, then  the _inverse document frequency_ is\n",
    "  $$ \\mathrm{idf}^{naive}(t,D) = \\log \\frac{\\# D}{\\# \\{d \\in D : t \\in d\\}} \\\\\n",
    "  = \\log \\frac{\\text{count of all documents}}{\\text{count of those documents containing the term $t$}} $$\n",
    "  with a common variant being\n",
    "  $$ \\mathrm{idf}(t, D) = \\log \\frac{\\# D}{1 + \\# \\{d \\in D : t \\in d\\}} \\\\\n",
    "   = \\log \\frac{\\text{count of all documents}}{1 + \\text{count of those documents containing the term $t$}} $$\n",
    "  (This second one is the default in scikit-learn. Without this tweak we would omit the $1+$ in the denominator and have to worry about dividing by zero if $t$ is not found in any documents.)\n",
    "  3. Finally, the weight that we assign to the term $t$ appearing in document $d$ and depending on the corpus of all documents $D$ is\n",
    "  $$ \\mathrm{tfidf}(t,d,D) = \\mathrm{tf}(t,d) \\mathrm{idf}(t,D) $$\n",
    "\n",
    "\n",
    "Putting it all together in Spark ML is very simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure an ML pipeline\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"CleanedArticle\", outputCol=\"words\")\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(),outputCol=\"filtered\")   \n",
    "hashingTF = HashingTF(inputCol=remover.getOutputCol(), outputCol=\"features\",numFeatures=1048576)\n",
    "#idf = IDF(inputCol=hashingTF.getOutputCol(),outputCol=\"features\")\n",
    "\n",
    "#Try KMeans and BisectingKMeans\n",
    "kmeans = KMeans().setK(2).setSeed(1) #.setMaxIter(50)\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, kmeans])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "model = pipeline.fit(data)\n",
    "\n",
    "# Make predictions on test documents and print columns of interest.\n",
    "prediction = model.transform(data)\n",
    "selected = prediction.select(\"NewsType\", \"CleanedArticle\", \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------+\n",
      "|NewsType|      CleanedArticle|prediction|\n",
      "+--------+--------------------+----------+\n",
      "|business|KARACHI: The Sind...|         0|\n",
      "|business|\"HONG KONG: Asian...|         1|\n",
      "|business|HONG KONG: Hong K...|         0|\n",
      "|business|\"HONG KONG: Asian...|         1|\n",
      "|business|\"NEW YORK: US oil...|         1|\n",
      "|business|\"New York: Oil pr...|         1|\n",
      "|business|KARACHI: Strong b...|         0|\n",
      "|business|\"Singapore: Oil f...|         1|\n",
      "|business|KARACHI: Wholesal...|         0|\n",
      "|business|\"SYDNEY: Oil pric...|         1|\n",
      "|business|TOKYO: Tokyo stoc...|         1|\n",
      "|business|HONG KONG: Hong K...|         0|\n",
      "|business|\"London: World oi...|         1|\n",
      "|business|\"ISLAMABAD: Long ...|         0|\n",
      "|business|\"SINGAPORE: Brent...|         1|\n",
      "|business|ISLAMABAD: A two ...|         0|\n",
      "|business|ISLAMABAD: The Na...|         0|\n",
      "|business|\"Hong Kong: Asian...|         1|\n",
      "|business|ISLAMABAD: The Ec...|         0|\n",
      "|business|\"SINGAPORE: Saudi...|         1|\n",
      "+--------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise***\n",
    " 1. Add **IDF** weighting to the pipeline\n",
    " 1. Try using `BisectingKMeans` instead of `KMeans` clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to try using n-grams instead of unigram features. And, perhaps, combine the two.\n",
    "The changes to the code are minimal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(),outputCol=\"filtered\")   \n",
    "ngram = NGram(n=2, inputCol=remover.getOutputCol(),outputCol=\"ngram\")\n",
    "hashingTF = HashingTF(inputCol=ngram.getOutputCol(), outputCol=\"bigrams\",numFeatures=1048576)\n",
    "#idf = IDF(inputCol=hashingTF.getOutputCol(),outputCol=\"features\")\n",
    "\n",
    "kmeans = KMeans().setK(4).setSeed(1) #.setMaxIter(50)\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, ngram, hashingTF, kmeans])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "model = pipeline.fit(data)\n",
    "\n",
    "# Make predictions on test documents and print columns of interest.\n",
    "prediction = model.transform(data)\n",
    "selected = prediction.select(\"id\", \"text\", \"prediction\")\n",
    "selected.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, combine the two using the `VectorAssembler` (this will be more memory consuming):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(),outputCol=\"filtered\")   \n",
    "ngram = NGram(n=2, inputCol=remover.getOutputCol(),outputCol=\"ngram\")\n",
    "hashingTF1 = HashingTF(inputCol=remover.getOutputCol(), outputCol=\"unigrams\") #,numFeatures=1048576)\n",
    "hashingTF2 = HashingTF(inputCol=ngram.getOutputCol(), outputCol=\"bigrams\") #,numFeatures=1048576)\n",
    "assembler = VectorAssembler(inputCols=[\"unigrams\", \"bigrams\"], outputCol=\"features\")\n",
    "#idf = IDF(inputCol=hashingTF.getOutputCol(),outputCol=\"features\")\n",
    "\n",
    "kmeans = KMeans().setK(4).setSeed(1) #.setMaxIter(50)\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, ngram, hashingTF1, hashingTF2, assembler, kmeans])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "model = pipeline.fit(data)\n",
    "\n",
    "# Make predictions on test documents and print columns of interest.\n",
    "prediction = model.transform(data)\n",
    "selected = prediction.select(\"id\", \"text\", \"prediction\")\n",
    "selected.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
