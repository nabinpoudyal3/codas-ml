{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed training\n",
    "\n",
    "\n",
    "Several algorithms for distributed training of neural networks exist. The are two main classes:\n",
    " 1. Data-parallel\n",
    " 1. Model-parallel\n",
    " \n",
    "**Data-parallel** approaches to distributed training keep a copy of the entire model on each worker, processing different subsets (mini-batches) of the training data set on each; while **model parallelism** assumes splitting a model between multiple workers (for instance, putting different layers on different GPUs). The latter is harder to implement.\n",
    "\n",
    "<img src=\"images/ModelDataParallelism.svg\">\n",
    "\n",
    "Data parallel training approaches all require some method of combining results and synchronizing the model parameters between each worker.\n",
    "\n",
    "For each of those classes one can distinguish between:\n",
    " 1. Asynchronous\n",
    " 1. Synchronous\n",
    " 1. n-soft synchronous\n",
    " \n",
    "\n",
    "## Synchronous training\n",
    "\n",
    "With synchronous approach in it's strictest form, often referred to as a **hard-synchronous** approach the training is done as follows:\n",
    "\n",
    " 1. Initialize the network parameters randomly based on the model configuration \n",
    " 1. Distribute a copy of the current parameters to each worker\n",
    " 1. Perform forward and backward passes on each worker using a mini-batch of data\n",
    " 1. Reduce the parameter to the master (parameter server), average them\n",
    " 1. Broadcast **global** parameters back to workers, return to the step 2. until there is more data\n",
    "\n",
    "<img src=\"images/ParameterAveraging.svg\">\n",
    "\n",
    "A variation of this approach assumes reducing the weight updates rather than weights.\n",
    "\n",
    "### Soft synchronization\n",
    "\n",
    "With the above approach, master has to collect all N weight updates. What if there are slow nodes / crashed nodes?\n",
    "Keeping this in mind, a more fault tolerant approach would require to collect a fraction of weight updates (normally > 90-95% of all) before proceeding to average.\n",
    "\n",
    "Implementation wise, this is normally done using a FIFO queue rather than a hard barrier: more about this later.\n",
    "\n",
    "## Asynchronous training\n",
    "\n",
    "In asynchronous algorithm the weight updates are applied as soon as they are available. Variations of this algorithm still perform synchronization, but only every n mini-batches.\n",
    "\n",
    "Among advantages of this approach are the speed of model convergence and low network communication overhead. The disadvantage would be the accuracy. The challenges of asynchronous training lay in handling the stale gradients - on average, the gradient staleness is equal to N - where N is the number of workers. Intuitively, one wants to minimize the impact of stale gradients during the weights update:\n",
    "$$W_{k} = W_{k-1} + \\lambda_{k}\\cdot \\Delta W$$ \n",
    "\n",
    "typically this is achieved by incorporating the gradient staleness into the learning rate schedule as $$W_{k} = W_{k-1} + \\frac{1}{N}\\lambda_{k} \\Delta W$$\n",
    "where N (number of workers) is the average gradient staleness.\n",
    "\n",
    "<img src=\"images/StaleGradients.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Sync replicas optimizer\n",
    "\n",
    "```python\n",
    "tf.train.SyncReplicasOptimizer\n",
    "```\n",
    "\n",
    "Is a class to synchronize, aggregate weight updates and pass them to the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
