{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data into RDD, DataFrame\n",
    "\n",
    "\n",
    "We will focus on structured data here for the sake of time, since our focus should be on machine learning.\n",
    "\n",
    "Loading unstructured data can be challanging, the lowest level API here is `textFile`, `wholeTextFiles` and `binaryRecords`. \n",
    "\n",
    "In some cases we would have to deal with data formats specific for a scientific collaboration. Example: CMS collaboration relies on root formats. Good news is, there are tools available for reading from root into Spark dataframes or RDDs.\n",
    "\n",
    "For now, let us assume that someone has done the work of preparing and structring the data for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading CSV\n",
    "\n",
    "First, we are going to learn how to load data into structured CSV format. There is at least two ways to do that:\n",
    "\n",
    "1) Read the files line by line with `textFiles()` method, splitting on a delimiter\n",
    "\n",
    "Similarly to Python, there is a data structured designed to be used when working with structured data (I mean Pandas Dataframes), it is also called the Dataframe (a concept closely linked to Spark SQL). There is a way to read CSV directly into Spark dataframe \n",
    "\n",
    "2) csv Dataframe reader\n",
    "\n",
    "Note: it used to be a third party spark-csv library developed and maintained by Databricks\n",
    "https://github.com/databricks/spark-csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def output_cleaner():\n",
    "    import os\n",
    "    os.system(\"rm -rf ./output*\")\n",
    "    print \"Output folders removed!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "import StringIO\n",
    "import os\n",
    "\n",
    "prepath = \"file://\"+os.environ.get(\"HOME\")+\"/codas-ml\"\n",
    "\n",
    "#this one is use when you use textFile\n",
    "def loadRecord(line,header,delimiter):\n",
    "    \"\"\"Parse a CSV line\"\"\"\n",
    "    input = StringIO.StringIO(line)\n",
    "    reader = csv.DictReader(input, delimiter=delimiter, fieldnames=header)\n",
    "    return next(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load into a regular RDD using textFile and parsing the CSV file line by line\n",
    "\n",
    "delimiter = \",\"\n",
    "\n",
    "# Try the record-per-line-input\n",
    "input = sc.textFile(prepath+\"/data/diamonds.csv\")\n",
    "header = input.first().split(delimiter)\n",
    "data = input.filter(lambda x: header[0] not in x).map(lambda x: loadRecord(x,header,delimiter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'\"color\"': 'E', u'\"\"': '1', u'\"carat\"': '0.23', u'\"cut\"': 'Ideal', u'\"table\"': '55', u'\"price\"': '326', u'\"clarity\"': 'SI2', u'\"depth\"': '61.5', u'\"x\"': '3.95', u'\"y\"': '3.98', u'\"z\"': '2.43'}\n",
      "{u'\"color\"': 'E', u'\"\"': '2', u'\"carat\"': '0.21', u'\"cut\"': 'Premium', u'\"table\"': '61', u'\"price\"': '326', u'\"clarity\"': 'SI1', u'\"depth\"': '59.8', u'\"x\"': '3.89', u'\"y\"': '3.84', u'\"z\"': '2.31'}\n",
      "{u'\"color\"': 'E', u'\"\"': '3', u'\"carat\"': '0.23', u'\"cut\"': 'Good', u'\"table\"': '65', u'\"price\"': '327', u'\"clarity\"': 'VS1', u'\"depth\"': '56.9', u'\"x\"': '4.05', u'\"y\"': '4.07', u'\"z\"': '2.31'}\n"
     ]
    }
   ],
   "source": [
    "for item in data.take(3):\n",
    "    print item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this can be later converted to the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV files to Dataframes (directly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Spark CSV dataframe reader can handle delimiters, escaping, and skipping header lines for CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read csv data as DataFrame using spark csv dataframe reader\n",
    "diamonds = spark.read.options(header='true', inferSchema='true').csv(prepath+\"/data/diamonds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---------+-----+-------+-----+-----+-----+----+----+----+\n",
      "|_c0|carat|      cut|color|clarity|depth|table|price|   x|   y|   z|\n",
      "+---+-----+---------+-----+-------+-----+-----+-----+----+----+----+\n",
      "|  1| 0.23|    Ideal|    E|    SI2| 61.5| 55.0|  326|3.95|3.98|2.43|\n",
      "|  2| 0.21|  Premium|    E|    SI1| 59.8| 61.0|  326|3.89|3.84|2.31|\n",
      "|  3| 0.23|     Good|    E|    VS1| 56.9| 65.0|  327|4.05|4.07|2.31|\n",
      "|  4| 0.29|  Premium|    I|    VS2| 62.4| 58.0|  334| 4.2|4.23|2.63|\n",
      "|  5| 0.31|     Good|    J|    SI2| 63.3| 58.0|  335|4.34|4.35|2.75|\n",
      "|  6| 0.24|Very Good|    J|   VVS2| 62.8| 57.0|  336|3.94|3.96|2.48|\n",
      "|  7| 0.24|Very Good|    I|   VVS1| 62.3| 57.0|  336|3.95|3.98|2.47|\n",
      "|  8| 0.26|Very Good|    H|    SI1| 61.9| 55.0|  337|4.07|4.11|2.53|\n",
      "|  9| 0.22|     Fair|    E|    VS2| 65.1| 61.0|  337|3.87|3.78|2.49|\n",
      "| 10| 0.23|Very Good|    H|    VS1| 59.4| 61.0|  338| 4.0|4.05|2.39|\n",
      "| 11|  0.3|     Good|    J|    SI1| 64.0| 55.0|  339|4.25|4.28|2.73|\n",
      "| 12| 0.23|    Ideal|    J|    VS1| 62.8| 56.0|  340|3.93| 3.9|2.46|\n",
      "| 13| 0.22|  Premium|    F|    SI1| 60.4| 61.0|  342|3.88|3.84|2.33|\n",
      "| 14| 0.31|    Ideal|    J|    SI2| 62.2| 54.0|  344|4.35|4.37|2.71|\n",
      "| 15|  0.2|  Premium|    E|    SI2| 60.2| 62.0|  345|3.79|3.75|2.27|\n",
      "| 16| 0.32|  Premium|    E|     I1| 60.9| 58.0|  345|4.38|4.42|2.68|\n",
      "| 17|  0.3|    Ideal|    I|    SI2| 62.0| 54.0|  348|4.31|4.34|2.68|\n",
      "| 18|  0.3|     Good|    J|    SI1| 63.4| 54.0|  351|4.23|4.29| 2.7|\n",
      "| 19|  0.3|     Good|    J|    SI1| 63.8| 56.0|  351|4.23|4.26|2.71|\n",
      "| 20|  0.3|Very Good|    J|    SI1| 62.7| 59.0|  351|4.21|4.27|2.66|\n",
      "+---+-----+---------+-----+-------+-----+-----+-----+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "diamonds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- carat: double (nullable = true)\n",
      " |-- cut: string (nullable = true)\n",
      " |-- color: string (nullable = true)\n",
      " |-- clarity: string (nullable = true)\n",
      " |-- depth: double (nullable = true)\n",
      " |-- table: double (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "diamonds.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Analyzing CSV files in Python as DataFrames\n",
    "\n",
    "Let's try doing some basic queries to understand the dataset better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53940"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diamonds.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|color|\n",
      "+-----+\n",
      "|    F|\n",
      "|    E|\n",
      "|    D|\n",
      "|    J|\n",
      "|    G|\n",
      "|    I|\n",
      "|    H|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "diamonds.select('color').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|carat|          avgPrice|\n",
      "+-----+------------------+\n",
      "| 3.51|           18701.0|\n",
      "| 2.67|           18686.0|\n",
      "|  4.5|           18531.0|\n",
      "| 5.01|           18018.0|\n",
      "| 2.57|17841.666666666668|\n",
      "|  2.6|           17535.0|\n",
      "| 2.64|           17407.0|\n",
      "| 4.13|           17329.0|\n",
      "| 2.39|17182.428571428572|\n",
      "| 2.71|           17146.0|\n",
      "+-----+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Convert Price column from integer to type DoubleType - we are going to do some arithmetics on it\n",
    "diamondsdf = diamonds.withColumn(\"price\", diamonds[\"price\"].cast(DoubleType()))\n",
    "\n",
    "# Calculate average price per carat value\n",
    "carat_avgPrice = (diamondsdf\n",
    "                  .groupBy(\"carat\")\n",
    "                  .avg(\"price\")\n",
    "                  .withColumnRenamed(\"avg(price)\", \"avgPrice\")\n",
    "                  .orderBy(desc(\"avgPrice\")))\n",
    "\n",
    "# View top10 highest average prices and corresponding carat value\n",
    "carat_avgPrice.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can convert the DataFrame directly into an RDD\n",
    "diamonds_rdd = diamonds.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=1, carat=0.23, cut=u'Ideal', color=u'E', clarity=u'SI2', depth=61.5, table=55.0, price=326, x=3.95, y=3.98, z=2.43),\n",
       " Row(_c0=2, carat=0.21, cut=u'Premium', color=u'E', clarity=u'SI1', depth=59.8, table=61.0, price=326, x=3.89, y=3.84, z=2.31),\n",
       " Row(_c0=3, carat=0.23, cut=u'Good', color=u'E', clarity=u'VS1', depth=56.9, table=65.0, price=327, x=4.05, y=4.07, z=2.31)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View first 3 rows of the diamonds RDD of rows\n",
    "diamonds_rdd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You can now use RDD operations to analyze the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Ideal', 21551),\n",
       " (u'Good', 4906),\n",
       " (u'Premium', 13791),\n",
       " (u'Very Good', 12082),\n",
       " (u'Fair', 1610)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Diamond counts by cuts\n",
    "countByGroup = diamonds_rdd.map(lambda x: (x.cut, 1)).reduceByKey(lambda x,y: x+y)\n",
    "countByGroup.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'SI2', u'SI1', u'VS1', u'I1', u'VS2', u'VVS1', u'VVS2', u'IF']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distinct diamond clarities in dataset\n",
    "distinctClarity = diamonds_rdd.map(lambda x: x.clarity).distinct()\n",
    "distinctClarity.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Ideal', 2756.7240663718817),\n",
       " (u'Good', 2755.647409027791),\n",
       " (u'Premium', 2756.654813661215),\n",
       " (u'Very Good', 2756.7183661747795),\n",
       " (u'Fair', 2743.567771968392)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average price per diamond cut\n",
    "avgPrice = diamonds_rdd.map(lambda x: (x.cut, float(x.price))).reduceByKey(lambda x,y: (x+y)/2)\n",
    "avgPrice.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON files with Python\n",
    "\n",
    "This notebook shows an example of how to load JSON data in Python notebooks and best practices for working with JSON data.\n",
    "\n",
    "#### Loading JSON data with Spark SQL into a DataFrame\n",
    "\n",
    "Spark SQL has built in support for reading in JSON files which contain a separate, self-contained JSON object per line. Multi-line JSON files are currently not compatible with Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testJsonData = spark.read.json(prepath+\"/data/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- array: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- dict: struct (nullable = true)\n",
      " |    |-- extra_key: string (nullable = true)\n",
      " |    |-- key: string (nullable = true)\n",
      " |-- int: long (nullable = true)\n",
      " |-- string: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testJsonData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+---+-------+\n",
      "|    array|                dict|int| string|\n",
      "+---------+--------------------+---+-------+\n",
      "|[1, 2, 3]|       [null,value1]|  1|string1|\n",
      "|[2, 4, 6]|       [null,value2]|  2|string2|\n",
      "|[3, 6, 9]|[extra_value3,val...|  3|string3|\n",
      "+---------+--------------------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testJsonData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Spark SQL can infer the schema automatically from your JSON data. To view the schema, use printSchema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parquet Files in Python\n",
    "\n",
    "This notebook describes how to register a table in Spark SQL from parquet files.\n",
    "Parquet Files are a great format for storing large tables in SparkSQL.\n",
    "Consider converting text files with a schema into parquet files for more efficient storage.\n",
    "Parquet provides a lot of optimizations under the hood to speed up your queries.\n",
    "Just call ```bash .write.parquet``` on a DataFrame to encode in into Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+-----------+---------------+-----+\n",
      "|     group|key|        map|       someints|value|\n",
      "+----------+---+-----------+---------------+-----+\n",
      "|    vowels|  a|Map(a -> 1)|            [1]|    1|\n",
      "|consonants|  b|Map(b -> 2)|         [2, 2]|    2|\n",
      "|consonants|  c|Map(c -> 3)|      [3, 3, 3]|    3|\n",
      "|consonants|  d|Map(d -> 4)|   [4, 4, 4, 4]|    4|\n",
      "|    vowels|  e|Map(3 -> 5)|[5, 5, 5, 5, 5]|    5|\n",
      "+----------+---+-----------+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "array = [Row(key=\"a\", group=\"vowels\", value=1, someints=[1], map = {\"a\" : 1}),\n",
    "         Row(key=\"b\", group=\"consonants\", value=2, someints=[2, 2], map = {\"b\" : 2}),\n",
    "         Row(key=\"c\", group=\"consonants\", value=3, someints=[3, 3, 3], map = {\"c\" : 3}),\n",
    "         Row(key=\"d\", group=\"consonants\", value=4, someints=[4, 4, 4, 4], map = {\"d\" : 4}),\n",
    "         Row(key=\"e\", group=\"vowels\", value=5, someints=[5, 5, 5, 5, 5], map = {\"3\" : 5})]\n",
    "dataframe = spark.createDataFrame(sc.parallelize(array))\n",
    "dataframe.show()\n",
    "# now that it's created, let's write it to disk\n",
    "dataframe.write.parquet(prepath+\"/output_parquet/testParquetFiles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p>Continue on to the next exercise: [2_DataFrames.ipynb](2_DataFrames.ipynb).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
